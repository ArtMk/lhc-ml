{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925ec8dc-bc27-46e1-8498-3803fe3d238a",
   "metadata": {},
   "source": [
    "# 5. Training in Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5b8ea-4e6a-4bc7-8f01-7dbb76158aa1",
   "metadata": {},
   "source": [
    "In the last notebook events from the Di-Higgs signal process and some of its background processes were selected which fulfilled a desired signiture consisting of lepton and jet counts and requirements on some physical observables. The selected data was then converted into a vector form which can be used as input by a neural network and saved into files.\n",
    "\n",
    "In this notebook a neural network will be constructed which is capable of taking the selected data from the last notebook as input and target values for the training will be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b9272d-6e01-456b-957d-a49ebd6d7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 13:55:02.035812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-22 13:55:02.035867: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b80bfff-fcd2-426c-bf95-bc0cf5730673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.32476807e+01 -2.20458984e+00 -1.55212402e-01  1.02250000e+02\n",
      "  -3.79394531e-01  1.07714844e+00  9.66308594e-01  4.77812500e+01\n",
      "  -9.70214844e-01  2.47705078e+00  9.99023438e-01  1.45375000e+02\n",
      "  -1.36816406e+00 -3.69140625e-01  3.84033203e-01]\n",
      " [ 4.98770027e+01 -1.43994141e+00  2.39062500e+00  2.18250000e+02\n",
      "  -1.46044922e+00  2.35888672e+00  8.22265625e-01  2.71093750e+01\n",
      "  -9.82299805e-01  1.75781250e+00  9.98046875e-01  1.30375000e+02\n",
      "  -1.87817383e+00 -1.58007812e+00  9.94873047e-02]]\n",
      "4344\n"
     ]
    }
   ],
   "source": [
    "# open the neural network input vectors\n",
    "\n",
    "nn_input = {}\n",
    "\n",
    "processes = ['signal', 'bgr_tt', 'bgr_st', 'bgr_Wj']\n",
    "\n",
    "for process in processes:\n",
    "    nn_input[process] = np.loadtxt('nn_input_' + process + '.txt', delimiter = ' ')\n",
    "    \n",
    "    # simplify data structure by transposing\n",
    "    nn_input[process] = np.transpose(nn_input[process])\n",
    "\n",
    "print(nn_input['signal'][:2])\n",
    "print(len(nn_input['signal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc47f6-ac7e-497e-82f0-a30cb7849608",
   "metadata": {},
   "source": [
    "Not only do we want to understand if the if the event in question is part of the signal or the background, we also want to be able to differentiate the various background processes. Essentially, we are building a *multiclassifier* neural network. This means that the network, instead of returning a binary \"signal\" or \"background\" response, returns an output vector of which each element is associated with the probablility of identifying the events as stemming from the various processes. In our case we have 4 processes that have to be distinguished from one another; an exemplary output vector of the shape (0 0 1 0) would indicate the ideal case of identifying an event as a single top background event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b58c7fc1-793d-445a-b7e3-dd6e7b28c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer structure of the network:  [15, 30, 30, 30, 4]\n"
     ]
    }
   ],
   "source": [
    "# define structure of the network\n",
    "\n",
    "# dimensions of input, hidden and output layers\n",
    "N0 = len(nn_input['signal'][0])\n",
    "N1 = 30\n",
    "N2 = len(processes)\n",
    "\n",
    "# number of hidden layers\n",
    "layer_count = 3\n",
    "\n",
    "# layer structure of the network\n",
    "layers = [N0]\n",
    "\n",
    "for l in range(layer_count):\n",
    "    layers.append(N1)\n",
    "    \n",
    "layers.append(N2)\n",
    "\n",
    "print(\"Layer structure of the network: \", layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4e4cc96-2cff-416c-931d-93b3e807f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the network in keras\n",
    "\n",
    "net = Sequential()\n",
    "\n",
    "# set the weights initializer\n",
    "initializer = RandomUniform(minval=-10, maxval=10)\n",
    "\n",
    "# make the first layer with the input shape as an argument\n",
    "net.add(Dense(layers[1], input_shape = (layers[0],), activation = 'relu', \n",
    "              use_bias = True, kernel_initializer = initializer))\n",
    "\n",
    "# make all the other layers\n",
    "for i in range(2, len(layers) - 1):\n",
    "    net.add(Dense(layers[i], activation = 'relu', use_bias = True, kernel_initializer = initializer))\n",
    "    \n",
    "net.add(Dense(layers[-1], activation = 'softmax', use_bias = True, kernel_initializer = initializer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0002f706-f9aa-42bd-8144-31b1e8ab8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the network\n",
    "\n",
    "net.compile(loss = 'CategoricalCrossentropy', optimizer = keras.optimizers.SGD(learning_rate = 0.5), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2c60fd7-ace0-482a-ae0f-8b474f6a5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making of the target output\n",
    "\n",
    "# target dictionary\n",
    "nn_target = {}\n",
    "\n",
    "training_duration = 0\n",
    "\n",
    "# make target data for a particular process\n",
    "def make_target(p, proc):\n",
    "    \n",
    "    vectors = []\n",
    "    \n",
    "    for i in range(len(nn_input[proc])):\n",
    "    \n",
    "        v = np.zeros(len(processes))\n",
    "        v[p] = 1\n",
    "\n",
    "        vectors.append(v)\n",
    "    \n",
    "    return vectors, len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "501a0c84-c243-4f16-903d-0fc97859c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# make target data for all processes\n",
    "for count, process in enumerate(processes):\n",
    "    nn_target[process] = make_target(count, process)[0]\n",
    "    training_duration += make_target(count, process)[1]\n",
    "\n",
    "print(nn_target['bgr_Wj'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbf5d3ae-5004-47e1-a423-c0a8c109f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost\n",
    "costs = np.empty(training_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c9e3b1e-5576-490c-a3c7-5cd37b5a32c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 15\n  y sizes: 4\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train network\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(nn_target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m----> 4\u001b[0m     costs[i] \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_target\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/my_conda_env/lib/python3.10/site-packages/keras/engine/training.py:2140\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2137\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   2138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[1;32m   2139\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2140\u001b[0m   iterator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m   2144\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n",
      "File \u001b[0;32m~/my_conda_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:1638\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m   data \u001b[38;5;241m=\u001b[39m (x, y, sample_weight)\n\u001b[0;32m-> 1638\u001b[0m \u001b[43m_check_data_cardinality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1639\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(data)\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight:\n",
      "File \u001b[0;32m~/my_conda_env/lib/python3.10/site-packages/keras/engine/data_adapter.py:1655\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1652\u001b[0m       label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1653\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)))\n\u001b[1;32m   1654\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1655\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 15\n  y sizes: 4\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# train network\n",
    "\n",
    "for i in range(len(nn_target['signal'])):\n",
    "    costs[i] = net.train_on_batch(nn_input['signal'][i], nn_target['signal'][i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d5277ac-44e2-4b29-8bd0-3431c28afc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4344\n"
     ]
    }
   ],
   "source": [
    "print(len(np.transpose(nn_input['signal'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bc607749-1dbd-458c-aca7-4d3529f25097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.32476807e+01 -2.20458984e+00 -1.55212402e-01  1.02250000e+02\n",
      "  -3.79394531e-01  1.07714844e+00  9.66308594e-01  4.77812500e+01\n",
      "  -9.70214844e-01  2.47705078e+00  9.99023438e-01  1.45375000e+02\n",
      "  -1.36816406e+00 -3.69140625e-01  3.84033203e-01]\n",
      " [ 4.98770027e+01 -1.43994141e+00  2.39062500e+00  2.18250000e+02\n",
      "  -1.46044922e+00  2.35888672e+00  8.22265625e-01  2.71093750e+01\n",
      "  -9.82299805e-01  1.75781250e+00  9.98046875e-01  1.30375000e+02\n",
      "  -1.87817383e+00 -1.58007812e+00  9.94873047e-02]]\n",
      "[array([1., 0., 0., 0.]), array([1., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(nn_input['signal'][:2])\n",
    "print(nn_target['signal'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "23a8cb6d-a2a2-42de-9910-821849ac4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4344, 15)\n",
      "(4344, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(np.reshape(nn_input['signal'], (4344, 15))))\n",
    "print(np.shape(nn_target['signal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "64654a5d-8c6b-418e-8d0a-a7feabbfe0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 33.24768066  -2.20458984  -0.1552124  102.25        -0.37939453\n",
      "   1.07714844   0.96630859  47.78125     -0.97021484   2.47705078\n",
      "   0.99902344 145.375       -1.36816406  -0.36914062   0.3840332 ]\n",
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(nn_input['signal'][0])\n",
    "print(nn_target['signal'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e490c0a-f250-4ac8-b15d-170eb0b160c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cms-analysis",
   "language": "python",
   "name": "cms-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
